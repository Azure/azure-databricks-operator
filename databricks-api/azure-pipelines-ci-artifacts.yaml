# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
  branches:
    include:
    - master
  paths:
    include:
    - databricks-api/*

variables:
  WORKING_DIR: 'databricks-api'
  IMAGE_NAME: 'candidate/azure-databricks/api:$(Build.SourceVersion)' 
  #ACR_NAME - set this value in AzureDevops variables 
  #AZURE_SUBSCRIPTION - set this value in AzureDevops variables
pool:
  vmImage: 'Ubuntu-16.04'

steps:
 
- task: UsePythonVersion@0
  inputs:
    versionSpec: '3.6'
    architecture: 'x64'

- script: pip install -r requirements-dev.txt && pip install -r requirements.txt
  workingDirectory: $(WORKING_DIR)
  displayName: 'Install requirements'

- script: flake8 
  workingDirectory: $(WORKING_DIR)
  displayName: 'Run lint tests'


- script: docker build -t $(IMAGE_NAME) .  # add options to this command to meet your needs
  condition: and(succeeded(), ne(variables['Build.SourceBranch'], 'refs/heads/master'))
  workingDirectory: $(WORKING_DIR)
  displayName: 'Docker build'


- task: AzureCLI@1
  displayName: Push to Azure Container Registry
  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/master'))  
  inputs:
    azureSubscription: $(AZURE_SUBSCRIPTION)
    scriptLocation: inlineScript 
    failOnStandardError: false
    workingDirectory: $(WORKING_DIR) 
    inlineScript: az acr build --registry $(ACR_NAME) --image $(IMAGE_NAME) . 

- script: echo $(IMAGE_NAME) > $(Build.ArtifactStagingDirectory)/databricks-api.txt
  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/master'))  

- task: PublishBuildArtifacts@1
  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/master'))  
  inputs:
    pathtoPublish: $(Build.ArtifactStagingDirectory)
    artifactName: drop
  
